{
    "data_options": {
        "data_path": "bAbI",
        "task_number": 19,
        "language": "en",
        "reader": "minibatch"
    },
    "log_options": {
        "dump_epoch": 5,
        "dump_path": "configs",
        "dump_name": "pyramid-attention-lstm.json",
        "dump_config": false
    },
    "optimization_options": {
        "learning_rate": 0.01,
        "batch_size_train": 64,
        "batch_size_test": null,
        "optimizer": "rmsprop",
        "max_epoch": 200,
        "verbose": true,
        "disp_iter": 20,
        "decay": 0.9,
        "decay_period": 5,
        "weight_path": "weights/pyramid-attention-lstm",
        "load_params": false,
        "load_name": "pyramid-attention-lstm.pkl",
        "dump_params": true,
        "dump_name": "pyramid-attention-lstm.pkl",
        "shuffle": true,
        "reg": null,
        "reg_weight": 0.0002,
        "dropout": true,
        "p_dropout": 0.5
    },
    "model_options": {
        "model_name": "pyramid-attention-lstm",
        "context_length": 16,
        "sentence_length": 16,
        "m_size": 64,
        "g_size": 64,
        "embedding_size": 20,
        "num_hid": 32,
        "num_hid_2": 32,
        "vocab_size": null,
        "sentence_level_att": true
    }
}
